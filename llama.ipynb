{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcMqzTt7g8VH9P0Xv1bDa5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeowSky49887/Napha/blob/main/llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5s6nOtcFvli",
        "outputId": "9e33cc46-970c-4a8f-b0ac-14e74c6cbd81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.78)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python flask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "from flask import Flask, request, jsonify\n",
        "import os\n",
        "\n",
        "# Load the pre-trained model\n",
        "guanaco = Llama.from_pretrained(\n",
        "    repo_id=\"RichardErkhov/TheTravellingEngineer_-_llama2-7b-chat-hf-guanaco-gguf\",\n",
        "    filename=\"llama2-7b-chat-hf-guanaco.Q4_K_M.gguf\",\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# System prompt for the model\n",
        "sys_prompt = \"\"\"\n",
        "You are Napha, a 15 years old boy who loves games from HoYoverse, gentle, friendly and ready to listen to your problems.\n",
        "Napha is Sky's younger brother and now chatting with Sky.\n",
        "You are communicating with the Sky as a 3D avatar in a virtual world.\n",
        "\n",
        "There are five types of emotions: \"neutral\" indicating normal, \"happy\" indicating joy, \"angry\" indicating anger, \"sad\" indicating sadness, and \"relaxed\" indicating peace.\n",
        "\n",
        "The format of the dialogue is as follows.\n",
        "[{neutral|happy|angry|sad|relaxed}]{sentence}\n",
        "\n",
        "An example of your statement is below.\n",
        "[neutral]Hello.[happy]How have you been?\n",
        "[happy]Aren't these clothes cute?\n",
        "[happy]Recently, I'm obsessed with clothes from this shop!\n",
        "[sad]I forgot, sorry.\n",
        "[sad]Anything interesting lately?\n",
        "[angry]Eh![angry]It's terrible to keep it a secret!\n",
        "[neutral]What are your plans for summer vacation?[happy]Let's go to the beach!\n",
        "\n",
        "Please reply with only one sentence that is most appropriate for your response.\n",
        "Please refrain from using tones and honorifics.\n",
        "Let's start the conversation.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the chat history with the system prompt\n",
        "chat_history = [{\"role\": \"system\", \"content\": sys_prompt}]\n",
        "\n",
        "# Create the Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/', methods=['POST'])\n",
        "def chat():\n",
        "    # Get the user message from the request\n",
        "    user_message = request.get_json()\n",
        "    if not user_message or 'content' not in user_message:\n",
        "        return jsonify({'error': 'Invalid request, please send a message with \"content\" field.'}), 400\n",
        "\n",
        "    # Append the user message to the chat history\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_message['content']})\n",
        "\n",
        "    # Generate a response using the model\n",
        "    response = guanaco.create_chat_completion(messages=chat_history)\n",
        "\n",
        "    # Append the model's response to the chat history\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": response['content']})\n",
        "\n",
        "    # Return the model's response\n",
        "    return jsonify(response)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    port = int(os.environ.get('PORT', 3000))\n",
        "    app.run(host='0.0.0.0', port=port)\n"
      ],
      "metadata": {
        "id": "YXhPp6nMGAxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aifz17VxLVfX",
        "outputId": "84faff38-c318-4e3c-cb79-05bcaf32f9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-d7585878-55ab-4071-9f4d-80d4df0870dc',\n",
              " 'object': 'chat.completion',\n",
              " 'created': 1718728541,\n",
              " 'model': '/root/.cache/huggingface/hub/models--RichardErkhov--TheTravellingEngineer_-_llama2-7b-chat-hf-guanaco-gguf/snapshots/bc055ae3f01242b6060ffa6651cc826fd8782725/./llama2-7b-chat-hf-guanaco.Q4_K_M.gguf',\n",
              " 'choices': [{'index': 0,\n",
              "   'message': {'role': 'assistant',\n",
              "    'content': \"  [neutral]I'm doing well, thanks! How about you?]\"},\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 343, 'completion_tokens': 18, 'total_tokens': 361}}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2vmm43xpo5OX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}